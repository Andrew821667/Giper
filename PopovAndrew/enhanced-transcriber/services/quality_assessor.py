"""
–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ 95%+
Quality assessment system for achieving 95%+ transcription quality
"""

import asyncio
import time
import logging
import re
from typing import Optional, Dict, Any, List, Tuple
from pathlib import Path
from collections import Counter
import statistics

from core.interfaces.quality_assessor import IQualityAssessor
from core.models.quality_metrics import QualityMetrics, QualityLevel, DomainAccuracy, DomainType
from core.models.config_models import ECommerceConfig

logger = logging.getLogger(__name__)


class QualityAssessmentService(IQualityAssessor):
    """
    –°–µ—Ä–≤–∏—Å –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    Quality assessment service for transcription evaluation
    """
    
    def __init__(
        self,
        use_reference_text: bool = False,
        enable_semantic_analysis: bool = True,
        enable_domain_analysis: bool = True,
        confidence_threshold: float = 0.8,
        ecommerce_config: Optional[ECommerceConfig] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        
        Args:
            use_reference_text: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è WER/CER
            enable_semantic_analysis: –í–∫–ª—é—á–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            enable_domain_analysis: –í–∫–ª—é—á–∏—Ç—å –¥–æ–º–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è low-confidence –∞–Ω–∞–ª–∏–∑–∞
            ecommerce_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è e-commerce –¥–æ–º–µ–Ω–∞
        """
        self.use_reference_text = use_reference_text
        self.enable_semantic_analysis = enable_semantic_analysis
        self.enable_domain_analysis = enable_domain_analysis
        self.confidence_threshold = confidence_threshold
        self.ecommerce_config = ecommerce_config or ECommerceConfig()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫
        self._check_dependencies()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self._initialize_models()
    
    def _check_dependencies(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        self.jiwer_available = False
        self.sentence_transformers_available = False
        self.nltk_available = False
        self.pymorphy2_available = False
        
        try:
            import jiwer
            self.jiwer_available = True
            logger.info("‚úÖ jiwer available for WER/CER calculation")
        except ImportError:
            logger.warning("‚ö†Ô∏è jiwer not available - WER/CER calculation disabled")
        
        try:
            from sentence_transformers import SentenceTransformer
            self.sentence_transformers_available = True
            logger.info("‚úÖ sentence-transformers available for semantic analysis")
        except ImportError:
            logger.warning("‚ö†Ô∏è sentence-transformers not available - semantic analysis disabled")
        
        try:
            import nltk
            self.nltk_available = True
            logger.info("‚úÖ nltk available for text analysis")
        except ImportError:
            logger.warning("‚ö†Ô∏è nltk not available - advanced text analysis disabled")
        
        try:
            import pymorphy2
            self.pymorphy2_available = True
            logger.info("‚úÖ pymorphy2 available for Russian morphology")
        except ImportError:
            logger.warning("‚ö†Ô∏è pymorphy2 not available - Russian morphology disabled")
    
    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        self.sentence_model = None
        self.morph_analyzer = None
        
        if self.sentence_transformers_available:
            try:
                from sentence_transformers import SentenceTransformer
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
                self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                logger.info("ü§ñ Semantic similarity model loaded")
            except Exception as e:
                logger.warning(f"Failed to load sentence transformer model: {e}")
        
        if self.pymorphy2_available:
            try:
                import pymorphy2
                self.morph_analyzer = pymorphy2.MorphAnalyzer()
                logger.info("üî§ Russian morphology analyzer loaded")
            except Exception as e:
                logger.warning(f"Failed to load pymorphy2: {e}")
    
    async def assess_quality(
        self,
        transcribed_text: str,
        audio_file: str,
        reference_text: Optional[str] = None,
        domain: str = "general",
        **kwargs
    ) -> QualityMetrics:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        
        Args:
            transcribed_text: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            audio_file: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            reference_text: –≠—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            domain: –î–æ–º–µ–Ω –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            QualityMetrics: –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        if not transcribed_text.strip():
            return self._create_empty_quality_metrics("Empty transcription text")
        
        start_time = time.time()
        logger.info(f"üìä Starting quality assessment for {len(transcribed_text)} characters")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
        metrics = QualityMetrics()
        
        try:
            # 1. –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ (WER/CER)
            if reference_text and self.jiwer_available:
                await self._calculate_error_rates(transcribed_text, reference_text, metrics)
            
            # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            if self.enable_semantic_analysis:
                await self._analyze_semantic_quality(transcribed_text, reference_text, metrics)
            
            # 3. –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            await self._analyze_text_quality(transcribed_text, metrics)
            
            # 4. –î–æ–º–µ–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            if self.enable_domain_analysis:
                await self._analyze_domain_accuracy(transcribed_text, domain, metrics)
            
            # 5. Confidence –∞–Ω–∞–ª–∏–∑
            word_confidences = kwargs.get('word_confidences', [])
            if word_confidences:
                await self._analyze_confidence_metrics(word_confidences, metrics)
            
            # 6. –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
            await self._analyze_content_metrics(transcribed_text, metrics)
            
            # 7. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            audio_duration = kwargs.get('audio_duration')
            if audio_duration:
                await self._analyze_temporal_metrics(transcribed_text, audio_duration, metrics)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            metrics.update_overall_assessment()
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
            metrics.evaluation_method = "automatic_comprehensive"
            metrics.reference_available = reference_text is not None
            
            processing_time = time.time() - start_time
            logger.info(
                f"‚úÖ Quality assessment completed: {metrics.overall_score:.3f} "
                f"({metrics.quality_level.value}) in {processing_time:.1f}s"
            )
            
            return metrics
            
        except Exception as e:
            logger.error(f"Quality assessment failed: {e}")
            return self._create_empty_quality_metrics(f"Assessment error: {str(e)}")
    
    async def _calculate_error_rates(self, transcribed: str, reference: str, metrics: QualityMetrics):
        """–†–∞—Å—á–µ—Ç WER –∏ CER"""
        try:
            import jiwer
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
            transcribed_norm = self._normalize_text_for_comparison(transcribed)
            reference_norm = self._normalize_text_for_comparison(reference)
            
            # Word Error Rate
            wer = jiwer.wer(reference_norm, transcribed_norm)
            metrics.word_error_rate = min(1.0, max(0.0, wer))
            
            # Character Error Rate  
            cer = jiwer.cer(reference_norm, transcribed_norm)
            metrics.character_error_rate = min(1.0, max(0.0, cer))
            
            logger.info(f"üìè Error rates: WER={wer:.3f}, CER={cer:.3f}")
            
        except Exception as e:
            logger.warning(f"Error rate calculation failed: {e}")
    
    async def _analyze_semantic_quality(self, transcribed: str, reference: Optional[str], metrics: QualityMetrics):
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞"""
        try:
            if not self.sentence_model or not reference:
                return
            
            # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
            transcribed_embedding = self.sentence_model.encode([transcribed])
            reference_embedding = self.sentence_model.encode([reference])
            
            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            from sklearn.metrics.pairwise import cosine_similarity
            similarity = cosine_similarity(transcribed_embedding, reference_embedding)[0][0]
            
            metrics.semantic_similarity = float(similarity)
            logger.info(f"üß† Semantic similarity: {similarity:.3f}")
            
        except Exception as e:
            logger.warning(f"Semantic analysis failed: {e}")
    
    async def _analyze_text_quality(self, text: str, metrics: QualityMetrics):
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Ç–µ–∫—Å—Ç–∞"""
        # –ë–µ–≥–ª–æ—Å—Ç—å —Ä–µ—á–∏ (fluency)
        metrics.fluency_score = self._calculate_fluency_score(text)
        
        # –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å (naturalness)
        metrics.naturalness_score = self._calculate_naturalness_score(text)
        
        # –ß–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å (readability)
        metrics.readability_score = self._calculate_readability_score(text)
        
        # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        metrics.punctuation_accuracy = self._calculate_punctuation_accuracy(text)
        
        # –ó–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã
        metrics.capitalization_accuracy = self._calculate_capitalization_accuracy(text)
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        metrics.sentence_structure_score = self._calculate_sentence_structure_score(text)
        
        logger.info(f"üìù Text quality: fluency={metrics.fluency_score:.2f}, naturalness={metrics.naturalness_score:.2f}")
    
    def _calculate_fluency_score(self, text: str) -> float:
        """–†–∞—Å—á–µ—Ç –±–µ–≥–ª–æ—Å—Ç–∏ —Ä–µ—á–∏"""
        sentences = self._split_into_sentences(text)
        if not sentences:
            return 0.0
        
        scores = []
        
        for sentence in sentences:
            words = sentence.split()
            if len(words) < 2:
                continue
            
            # –§–∞–∫—Ç–æ—Ä—ã –±–µ–≥–ª–æ—Å—Ç–∏
            factors = []
            
            # –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è 8-20 —Å–ª–æ–≤)
            length_score = self._score_sentence_length(len(words))
            factors.append(length_score)
            
            # –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Å–ª–æ–≤
            repetition_score = self._score_word_repetitions(words)
            factors.append(repetition_score)
            
            # –ü–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏
            transition_score = self._score_word_transitions(words)
            factors.append(transition_score)
            
            if factors:
                scores.append(statistics.mean(factors))
        
        return statistics.mean(scores) if scores else 0.5
    
    def _calculate_naturalness_score(self, text: str) -> float:
        """–†–∞—Å—á–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        # –§–∞–∫—Ç–æ—Ä—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
        factors = []
        
        # –ù–∞–ª–∏—á–∏–µ —Ñ–∏–ª–ª–µ—Ä–æ–≤ –∏ –ø–∞—Ä–∞–∑–∏—Ç–æ–≤ —Å–ª–æ–≤
        filler_score = self._score_filler_words(text)
        factors.append(filler_score)
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–ª–æ–≤–∞—Ä—è
        vocabulary_score = self._score_vocabulary_diversity(text)
        factors.append(vocabulary_score)
        
        # –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å (–±–∞–∑–æ–≤–∞—è)
        grammar_score = self._score_basic_grammar(text)
        factors.append(grammar_score)
        
        return statistics.mean(factors) if factors else 0.5
    
    def _calculate_readability_score(self, text: str) -> float:
        """–†–∞—Å—á–µ—Ç —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏"""
        sentences = self._split_into_sentences(text)
        words = text.split()
        
        if not sentences or not words:
            return 0.0
        
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        avg_sentence_length = len(words) / len(sentences)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        if 5 <= avg_sentence_length <= 20:
            length_score = 1.0
        elif avg_sentence_length < 5:
            length_score = avg_sentence_length / 5.0
        else:
            length_score = max(0.3, 20.0 / avg_sentence_length)
        
        # –°–ª–æ–∂–Ω–æ—Å—Ç—å —Å–ª–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–∞—è)
        complexity_score = self._score_word_complexity(words)
        
        return (length_score + complexity_score) / 2.0
    
    def _calculate_punctuation_accuracy(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏"""
        sentences = self._split_into_sentences(text)
        if not sentences:
            return 0.0
        
        correct_punctuation = 0
        total_sentences = len(sentences)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if sentence.endswith(('.', '!', '?')):
                correct_punctuation += 1
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø—è—Ç—ã—Ö (–±–∞–∑–æ–≤–∞—è)
            if self._has_reasonable_comma_usage(sentence):
                correct_punctuation += 0.5
        
        return min(1.0, correct_punctuation / total_sentences) if total_sentences > 0 else 0.0
    
    def _calculate_capitalization_accuracy(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤"""
        sentences = self._split_into_sentences(text)
        if not sentences:
            return 0.0
        
        correct_capitalization = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # –ü–µ—Ä–≤–∞—è –±—É–∫–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–∞–≥–ª–∞–≤–Ω–æ–π
            if sentence[0].isupper():
                correct_capitalization += 1
        
        return correct_capitalization / len(sentences) if sentences else 0.0
    
    def _calculate_sentence_structure_score(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        sentences = self._split_into_sentences(text)
        if not sentences:
            return 0.0
        
        structure_scores = []
        
        for sentence in sentences:
            words = sentence.split()
            if len(words) < 2:
                structure_scores.append(0.3)
                continue
            
            # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            score = 0.7  # –ë–∞–∑–æ–≤—ã–π score
            
            # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –≥–ª–∞–≥–æ–ª–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
            if self._has_verb_like_words(words):
                score += 0.2
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if len(words) < 3:
                score -= 0.3
            elif len(words) > 30:
                score -= 0.2
            
            structure_scores.append(max(0.0, min(1.0, score)))
        
        return statistics.mean(structure_scores)
    
    async def _analyze_domain_accuracy(self, text: str, domain: str, metrics: QualityMetrics):
        """–ê–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        if domain == "ecommerce":
            domain_accuracy = await self._analyze_ecommerce_accuracy(text)
            metrics.domain_accuracy = domain_accuracy
        
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –¥–æ–º–µ–Ω—ã
    
    async def _analyze_ecommerce_accuracy(self, text: str) -> DomainAccuracy:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ—á–Ω–æ—Å—Ç–∏ e-commerce —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏"""
        text_lower = text.lower()
        
        # –ü–æ–∏—Å–∫ e-commerce —Ç–µ—Ä–º–∏–Ω–æ–≤
        ecommerce_terms = self.ecommerce_config.get_default_ecommerce_terms()
        
        total_found = 0
        correct_terms = 0
        incorrect_terms = 0
        correct_list = []
        incorrect_list = []
        
        for correct_term, wrong_variants in ecommerce_terms.items():
            # –ü–æ–∏—Å–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
            if correct_term in text_lower:
                total_found += 1
                correct_terms += 1
                correct_list.append(correct_term)
            
            # –ü–æ–∏—Å–∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            for wrong_variant in wrong_variants:
                if wrong_variant in text_lower:
                    total_found += 1
                    incorrect_terms += 1
                    incorrect_list.append({
                        "found": wrong_variant,
                        "expected": correct_term
                    })
        
        # –†–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏
        if total_found > 0:
            accuracy_score = correct_terms / total_found
        else:
            accuracy_score = 1.0  # –ï—Å–ª–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤ –Ω–µ—Ç, —Ç–æ –æ—à–∏–±–æ–∫ —Ç–æ–∂–µ –Ω–µ—Ç
        
        return DomainAccuracy(
            domain=DomainType.ECOMMERCE,
            total_terms_found=total_found,
            correct_terms=correct_terms,
            incorrect_terms=incorrect_terms,
            missed_terms=0,  # –°–ª–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–∞
            accuracy_score=accuracy_score,
            correct_terms_list=correct_list,
            incorrect_terms_list=incorrect_list
        )
    
    async def _analyze_confidence_metrics(self, word_confidences: List[float], metrics: QualityMetrics):
        """–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        if not word_confidences:
            return
        
        # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        metrics.average_word_confidence = statistics.mean(word_confidences)
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        low_confidence_words = [c for c in word_confidences if c < self.confidence_threshold]
        metrics.low_confidence_words_count = len(low_confidence_words)
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç —Å–ª–æ–≤ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        if word_confidences:
            metrics.low_confidence_percentage = len(low_confidence_words) / len(word_confidences)
    
    async def _analyze_content_metrics(self, text: str, metrics: QualityMetrics):
        """–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è"""
        words = text.split()
        
        metrics.word_count = len(words)
        
        if words:
            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            unique_words = set(word.lower() for word in words)
            metrics.unique_words_count = len(unique_words)
            
            # –ë–æ–≥–∞—Ç—Å—Ç–≤–æ —Å–ª–æ–≤–∞—Ä—è
            metrics.vocabulary_richness = len(unique_words) / len(words)
    
    async def _analyze_temporal_metrics(self, text: str, audio_duration: float, metrics: QualityMetrics):
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        words = text.split()
        
        if audio_duration > 0 and words:
            # –°–∫–æ—Ä–æ—Å—Ç—å —Ä–µ—á–∏ –≤ —Å–ª–æ–≤–∞—Ö –≤ –º–∏–Ω—É—Ç—É
            metrics.speech_rate_wpm = (len(words) / audio_duration) * 60
    
    def _normalize_text_for_comparison(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è WER/CER"""
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _score_sentence_length(self, length: int) -> float:
        """–û—Ü–µ–Ω–∫–∞ –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        if 8 <= length <= 20:
            return 1.0
        elif length < 8:
            return length / 8.0
        else:
            return max(0.3, 20.0 / length)
    
    def _score_word_repetitions(self, words: List[str]) -> float:
        """–û—Ü–µ–Ω–∫–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π —Å–ª–æ–≤"""
        if len(words) < 2:
            return 1.0
        
        word_counts = Counter(word.lower() for word in words)
        repeated_words = sum(1 for count in word_counts.values() if count > 1)
        
        # –ß–µ–º –º–µ–Ω—å—à–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π, —Ç–µ–º –ª—É—á—à–µ
        repetition_ratio = repeated_words / len(word_counts)
        return max(0.0, 1.0 - repetition_ratio)
    
    def _score_word_transitions(self, words: List[str]) -> float:
        """–û—Ü–µ–Ω–∫–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ - –∏–∑–±–µ–≥–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö —Å–ª–æ–≤
        if len(words) < 2:
            return 1.0
        
        adjacent_repeats = 0
        for i in range(1, len(words)):
            if words[i].lower() == words[i-1].lower():
                adjacent_repeats += 1
        
        return max(0.0, 1.0 - (adjacent_repeats / len(words)))
    
    def _score_filler_words(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ñ–∏–ª–ª–µ—Ä-—Å–ª–æ–≤"""
        text_lower = text.lower()
        
        # –†—É—Å—Å–∫–∏–µ —Ñ–∏–ª–ª–µ—Ä—ã
        russian_fillers = [
            '—ç—ç', '–º–º', '–∞—Ö', '—ç—Ö', '–Ω—É', '–≤–æ—Ç', '—ç—Ç–æ —Å–∞–º–æ–µ', 
            '–∫–∞–∫ –±—ã', '—Ç–∏–ø–∞', '–∫–æ—Ä–æ—á–µ', '–±–ª–∏–Ω'
        ]
        
        filler_count = 0
        for filler in russian_fillers:
            filler_count += text_lower.count(filler)
        
        words = text.split()
        if not words:
            return 1.0
        
        filler_ratio = filler_count / len(words)
        return max(0.0, 1.0 - filler_ratio * 2)  # –®—Ç—Ä–∞—Ñ –∑–∞ —Ñ–∏–ª–ª–µ—Ä—ã
    
    def _score_vocabulary_diversity(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å–ª–æ–≤–∞—Ä—è"""
        words = [word.lower() for word in text.split()]
        if len(words) < 2:
            return 0.5
        
        unique_words = set(words)
        diversity_ratio = len(unique_words) / len(words)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (—Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–æ–∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–ª–æ—Ö–æ)
        if diversity_ratio > 0.8:
            return 0.8 + (diversity_ratio - 0.8) * 0.5
        return diversity_ratio
    
    def _score_basic_grammar(self, text: str) -> float:
        """–ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
        sentences = self._split_into_sentences(text)
        if not sentences:
            return 0.0
        
        grammar_score = 0.7  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        
        for sentence in sentences:
            words = sentence.split()
            if not words:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø–æ–¥–ª–µ–∂–∞—â–µ–≥–æ –∏ —Å–∫–∞–∑—É–µ–º–æ–≥–æ (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
            if len(words) >= 2:
                grammar_score += 0.1
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if len(words) < 2:
                grammar_score -= 0.2
        
        return max(0.0, min(1.0, grammar_score))
    
    def _score_word_complexity(self, words: List[str]) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤"""
        if not words:
            return 0.5
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ - —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤
        avg_length = statistics.mean(len(word) for word in words)
        
        # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤ 4-8 —Å–∏–º–≤–æ–ª–æ–≤
        if 4 <= avg_length <= 8:
            return 1.0
        elif avg_length < 4:
            return avg_length / 4.0
        else:
            return max(0.3, 8.0 / avg_length)
    
    def _has_reasonable_comma_usage(self, sentence: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑—É–º–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞–ø—è—Ç—ã—Ö"""
        comma_count = sentence.count(',')
        word_count = len(sentence.split())
        
        if word_count < 5:
            return comma_count == 0
        
        # –ü—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–Ω–∞ –∑–∞–ø—è—Ç–∞—è –Ω–∞ 8-10 —Å–ª–æ–≤
        expected_commas = word_count // 10
        return abs(comma_count - expected_commas) <= 1
    
    def _has_verb_like_words(self, words: List[str]) -> bool:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≥–ª–∞–≥–æ–ª–æ–≤"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        verb_endings = ['—Ç—å', '–µ—Ç', '–∏—Ç', '—é—Ç', '—è—Ç', '–∞–ª', '–ª–∞', '–ª–æ', '–ª–∏']
        
        for word in words:
            word_lower = word.lower()
            for ending in verb_endings:
                if word_lower.endswith(ending):
                    return True
        return False
    
    def calculate_wer(self, hypothesis: str, reference: str) -> float:
        """
        –†–∞—Å—á–µ—Ç Word Error Rate (WER)
        Calculate Word Error Rate
        
        Args:
            hypothesis: –¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            reference: –≠—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            float: WER –∑–Ω–∞—á–µ–Ω–∏–µ (0-1, –≥–¥–µ 0 - –∏–¥–µ–∞–ª—å–Ω–æ)
        """
        if not self.jiwer_available:
            logger.warning("jiwer not available, using simple WER calculation")
            return self._simple_wer_calculation(hypothesis, reference)
        
        try:
            import jiwer
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
            hypothesis_norm = self._normalize_text_for_comparison(hypothesis)
            reference_norm = self._normalize_text_for_comparison(reference)
            
            # –†–∞—Å—á–µ—Ç WER
            wer = jiwer.wer(reference_norm, hypothesis_norm)
            return min(1.0, max(0.0, wer))
            
        except Exception as e:
            logger.error(f"WER calculation failed: {e}")
            return self._simple_wer_calculation(hypothesis, reference)
    
    def calculate_cer(self, hypothesis: str, reference: str) -> float:
        """
        –†–∞—Å—á–µ—Ç Character Error Rate (CER)
        Calculate Character Error Rate
        
        Args:
            hypothesis: –¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            reference: –≠—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            float: CER –∑–Ω–∞—á–µ–Ω–∏–µ (0-1, –≥–¥–µ 0 - –∏–¥–µ–∞–ª—å–Ω–æ)
        """
        if not self.jiwer_available:
            logger.warning("jiwer not available, using simple CER calculation")
            return self._simple_cer_calculation(hypothesis, reference)
        
        try:
            import jiwer
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
            hypothesis_norm = self._normalize_text_for_comparison(hypothesis)
            reference_norm = self._normalize_text_for_comparison(reference)
            
            # –†–∞—Å—á–µ—Ç CER
            cer = jiwer.cer(reference_norm, hypothesis_norm)
            return min(1.0, max(0.0, cer))
            
        except Exception as e:
            logger.error(f"CER calculation failed: {e}")
            return self._simple_cer_calculation(hypothesis, reference)
    
    def assess_fluency(self, text: str, language: str = "ru") -> float:
        """
        –û—Ü–µ–Ω–∫–∞ –±–µ–≥–ª–æ—Å—Ç–∏ –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
        Assess text fluency and naturalness
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            float: –û—Ü–µ–Ω–∫–∞ –±–µ–≥–ª–æ—Å—Ç–∏ (0-1, –≥–¥–µ 1 - –æ—Ç–ª–∏—á–Ω–æ)
        """
        if not text.strip():
            return 0.0
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–∞—Å—á–µ—Ç–∞ –±–µ–≥–ª–æ—Å—Ç–∏
            fluency_score = self._calculate_fluency_score(text)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
            if language == "ru":
                # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —É—á–∏—Ç—ã–≤–∞–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é
                if self.morph_analyzer:
                    morphology_bonus = self._assess_russian_morphology(text)
                    fluency_score = (fluency_score + morphology_bonus) / 2.0
            
            return min(1.0, max(0.0, fluency_score))
            
        except Exception as e:
            logger.error(f"Fluency assessment failed: {e}")
            return 0.5  # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    def assess_domain_accuracy(self, text: str, domain: str) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ–º–µ–Ω–Ω–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏
        Assess domain-specific terminology accuracy
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            domain: –î–æ–º–µ–Ω (ecommerce, medical, legal, etc.)
            
        Returns:
            float: –¢–æ—á–Ω–æ—Å—Ç—å –¥–æ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (0-1)
        """
        if not text.strip():
            return 0.0
        
        try:
            if domain.lower() == "ecommerce":
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è e-commerce
                import asyncio
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    domain_accuracy = loop.run_until_complete(self._analyze_ecommerce_accuracy(text))
                    return domain_accuracy.accuracy_score
                finally:
                    loop.close()
            
            elif domain.lower() == "general":
                # –û–±—â–∏–π –¥–æ–º–µ–Ω - –±–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
                return self._assess_general_domain_accuracy(text)
            
            else:
                # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É
                logger.warning(f"Unknown domain '{domain}', using general assessment")
                return self._assess_general_domain_accuracy(text)
                
        except Exception as e:
            logger.error(f"Domain accuracy assessment failed for domain '{domain}': {e}")
            return 0.5  # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    def _simple_wer_calculation(self, hypothesis: str, reference: str) -> float:
        """
        –ü—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç WER –±–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ jiwer
        Simple WER calculation without jiwer library
        """
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
            hyp_words = self._normalize_text_for_comparison(hypothesis).split()
            ref_words = self._normalize_text_for_comparison(reference).split()
            
            if not ref_words:
                return 1.0 if hyp_words else 0.0
            
            # –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤
            hyp_set = set(hyp_words)
            ref_set = set(ref_words)
            
            # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ WER
            common_words = hyp_set & ref_set
            wer = 1.0 - (len(common_words) / len(ref_set))
            
            return min(1.0, max(0.0, wer))
            
        except Exception as e:
            logger.error(f"Simple WER calculation failed: {e}")
            return 1.0
    
    def _simple_cer_calculation(self, hypothesis: str, reference: str) -> float:
        """
        –ü—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç CER –±–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ jiwer
        Simple CER calculation without jiwer library
        """
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
            hyp_chars = set(self._normalize_text_for_comparison(hypothesis).replace(' ', ''))
            ref_chars = set(self._normalize_text_for_comparison(reference).replace(' ', ''))
            
            if not ref_chars:
                return 1.0 if hyp_chars else 0.0
            
            # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ CER
            common_chars = hyp_chars & ref_chars
            cer = 1.0 - (len(common_chars) / len(ref_chars))
            
            return min(1.0, max(0.0, cer))
            
        except Exception as e:
            logger.error(f"Simple CER calculation failed: {e}")
            return 1.0
    
    def _assess_russian_morphology(self, text: str) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ —Ä—É—Å—Å–∫–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ —Å pymorphy2
        Assess Russian morphology with pymorphy2
        """
        if not self.morph_analyzer:
            return 0.5
        
        try:
            words = text.split()
            if not words:
                return 0.0
            
            morphology_scores = []
            
            for word in words[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                # –û—á–∏—Å—Ç–∫–∞ —Å–ª–æ–≤–∞ –æ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
                clean_word = re.sub(r'[^\w]', '', word.lower())
                if not clean_word:
                    continue
                
                # –ê–Ω–∞–ª–∏–∑ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏
                parsed = self.morph_analyzer.parse(clean_word)
                if parsed:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞
                    best_parse = parsed[0]
                    score = best_parse.score if hasattr(best_parse, 'score') else 0.7
                    morphology_scores.append(score)
            
            return statistics.mean(morphology_scores) if morphology_scores else 0.5
            
        except Exception as e:
            logger.error(f"Russian morphology assessment failed: {e}")
            return 0.5
    
    def _assess_general_domain_accuracy(self, text: str) -> float:
        """
        –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –æ–±—â–µ–≥–æ –¥–æ–º–µ–Ω–∞
        Basic assessment for general domain
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
        factors = []
        
        # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
        words = text.split()
        if words:
            length_factor = min(1.0, len(words) / 20.0)  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ 20+ —Å–ª–æ–≤
            factors.append(length_factor)
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–ª–æ–≤–∞—Ä—è
        if words:
            unique_words = set(word.lower() for word in words)
            diversity = len(unique_words) / len(words)
            factors.append(min(1.0, diversity * 2.0))
        
        # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —è–≤–Ω—ã—Ö –æ—à–∏–±–æ–∫ (–±–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        error_indicators = ['—ç—ç', '–º–º', '—ç—ç—ç', '–º–º–º', '—Ç–µ—Å—Ç', '–ø—Ä–æ–≤–µ—Ä–∫–∞']
        text_lower = text.lower()
        error_count = sum(1 for indicator in error_indicators if indicator in text_lower)
        error_factor = max(0.0, 1.0 - error_count * 0.2)
        factors.append(error_factor)
        
        return statistics.mean(factors) if factors else 0.5
    
    def _create_empty_quality_metrics(self, error_message: str) -> QualityMetrics:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç—ã—Ö –º–µ—Ç—Ä–∏–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        metrics = QualityMetrics()
        metrics.overall_score = 0.0
        metrics.quality_level = QualityLevel.VERY_POOR
        metrics.evaluation_method = "failed"
        metrics.improvement_suggestions = [error_message]
        return metrics